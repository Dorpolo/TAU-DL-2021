{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0971b9f-9933-4489-978b-ef0193910d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn, optim as optim, functional as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed45c3-81bb-466a-93a0-d0d05fc7a543",
   "metadata": {},
   "source": [
    "# A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22d0e51-5258-4373-b5fb-f9501982e22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd472cf0290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "register_matplotlib_converters()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c52cdd-a1f4-4671-a080-aba04d5081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [(math.sqrt(99))/10, -1/10],\n",
    "    [1/10, (math.sqrt(99))/10]\n",
    "    ])\n",
    "\n",
    "B = np.array([\n",
    "    [math.sqrt(2)/2,-2/2],\n",
    "    [math.sqrt(2)/2,math.sqrt(2)/2]\n",
    "])\n",
    "\n",
    "\n",
    "eps1 = np.random.normal(loc=0, scale=0.2, size=[2, 2000])\n",
    "eps2 = np.random.normal(loc=0, scale=0.2**2, size=[2, 2000])\n",
    "\n",
    "\n",
    "x0 = np.array([1,0])+eps1[:,0] #do we need to add noise to XO?\n",
    "y0 = B.dot(x0) + eps2[:,0] #not true- how do w know the real YO?\n",
    "\n",
    "length = 2000\n",
    "train_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "x, y_hat = [x0], [y0]\n",
    "\n",
    "for i in range (length-1):\n",
    "  # X(k+1) = AX(k) + ϵ1(k+1)\n",
    "  x_next = a.dot(x[i]) + eps1[:, i+1]\n",
    "  x.append(x_next)\n",
    "  #Y(k+1) = BX(k+1) + ϵ2(k+1)\n",
    "  y_next = b.dot(x_next) + eps2[:, i+1]\n",
    "  y_hat.append(y_next)\n",
    "  i = i+1\n",
    "\n",
    "#need to update x_hat[i] accordingly\n",
    "X = pd.DataFrame(x)\n",
    "y = pd.DataFrame(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d9ac6-77b1-4347-9745-f4855a11059b",
   "metadata": {},
   "source": [
    "LSTM cells:\n",
    "\n",
    "\n",
    "*   C*t-1*- Cell state/Long-term memory- (last time stemp output)\n",
    "*   Hidden-State (Short term memmory)- H*t-1*\n",
    "*   Input- Y*t-1* (last time stemp output)\n",
    "\n",
    "\n",
    "INITILAZATIONS:\n",
    "H0,C0- tensors of Zeroes. in the backpopagation stage the network will learn these states by vy theire deriviations according to the loss functions.\n",
    "\n",
    "Stages:\n",
    "1.  Forget gate- Values between 0 to 1=> sigmoid(W *forget* (H*t-1*,Y*t-1*)+BIAS *forget* ) * C*t-1*\n",
    "\n",
    "2.  Input gate- - Values between -1 to 1==> sigmoid(Wi1(H*t-1*,Y*t-1*)+BIASi1) * tanjh(Wi2(H*t-1*,Y*t-1*)+BIASi2)\n",
    "\n",
    ">>The outcome represents the information to be kept in the long-term memory and used as the output. As the layer is being trained through back-propagation, the weights in the sigmoid function will be updated such that it learns to only let the useful pass through while discarding the less critical features\n",
    "\n",
    "3.  1+2: will define our C**t** value (New Long term memmory)\n",
    "\n",
    "4.  sigmoid(W output1 (H*t-1*,Y*t-1*)+BIAS output1) * tan (W output1(C**t**) + BIAS output2)- will define our output (Y_hat **t**) and out new Hidden state (H_hat **t**)  \n",
    ">First, the previous short-term memory and current input (H*t-1*,Y*t-1*) will be passed into a sigmoid function (Yes, this is the 3rd time we’re doing this) with different weights yet again to create the third and final filter. Then, we put the new long-term memory (C**t**) through an activation tanh function. The output from these 2 processes will be multiplied to produce the new short-term memory ( H**t**). The short-term ( H**t**) and long-term memory (C**t**) produced by these gates will then be carried over to the next cell for the process to be repeated. The output of each time step can be obtained ( Y**t**) from the short-term memory, also known as the hidden state.\n",
    "\n",
    "#https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc01e2-1885-422f-8433-51235003e724",
   "metadata": {},
   "source": [
    "## Building the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1f3108-0efb-4b77-b9aa-98791149a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_val_test_split(X, y,val_ratio, test_ratio):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X , y, 0.2, 0.5)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e395ae1-4871-44da-82e9-1d5846a5d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = len(y_train.columns)\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "batch_size =50\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr) #torch.Size([800, 2])\n",
    "train_targets = torch.Tensor(y_train_arr) #torch.Size([800, 2])\n",
    "val_features = torch.Tensor(X_val_arr)  #torch.Size([200, 2])\n",
    "val_targets = torch.Tensor(y_val_arr)  #torch.Size([200, 2])\n",
    "test_features = torch.Tensor(X_test_arr)  #torch.Size([1000, 2])\n",
    "test_targets = torch.Tensor(y_test_arr) \n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69824510-a0f1-4c3d-815d-74c0ce4fddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        # h0 = torch(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        #print('hn shape:', hn.shape, 'hn:',hn,'cn shape:', cn.shape, 'cn:',cn )\n",
    "        print('hn shape:', hn.shape)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size) so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (hidden_dim, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        # \"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel#,\n",
    "        # \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c222986-e100-4b04-bc8f-f0a4b72efe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "   \n",
    "    def train_step(self, x, y): #for single batch\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions per batch (x is the batch)\n",
    "        yhat = self.model(x)\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "        # print(\"y\",y, \"Yhat\",yhat,\"Loss\",loss)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=10, n_epochs=50, n_features=2):\n",
    "            # model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "            model_path = f\"{os.getcwd()}\"\n",
    "\n",
    "            for epoch in range(1, n_epochs + 1):   \n",
    "\n",
    "              #for train set:\n",
    "                batch_losses = []                      \n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "                    loss = self.train_step(x_batch, y_batch)\n",
    "                    batch_losses.append(loss)\n",
    "                training_loss = np.mean(batch_losses)\n",
    "                print ('batch_loses_size',len(batch_losses))\n",
    "                self.train_losses.append(training_loss)\n",
    "\n",
    "              #for val: torch.no_grad() means that we dont want to calculate gradient for the backpropagation- because it's validation/test set and not train set  \n",
    "                with torch.no_grad(): \n",
    "                    batch_val_losses = []\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                        y_val = y_val.to(device)\n",
    "                        self.model.eval()\n",
    "                        yhat = self.model(x_val)\n",
    "                        val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                        batch_val_losses.append(val_loss)\n",
    "                    validation_loss = np.mean(batch_val_losses)\n",
    "                    self.val_losses.append(validation_loss)\n",
    "\n",
    "                if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                    print(f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\")\n",
    "\n",
    "            torch.save(self.model.state_dict(), model_path)\n",
    "    \n",
    "    def plot_losses(self):\n",
    "          plt.plot(self.train_losses, label=\"Training loss\")\n",
    "          plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "          plt.legend()\n",
    "          plt.title(\"Losses\")\n",
    "          plt.show()\n",
    "          plt.close()\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=2):\n",
    "      with torch.no_grad(): #torch.no_grad() means that we dont want to calculate gradient for the backpropagation- because it's validation/test set and not train set\n",
    "          predictions = []\n",
    "          values = []\n",
    "          test_error = []\n",
    "\n",
    "          for x_test, y_test in test_loader:\n",
    "              x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "              y_test = y_test.to(device)\n",
    "              self.model.eval()\n",
    "              yhat = self.model(x_test)\n",
    "              predictions.append(yhat.to(device).detach().numpy())\n",
    "              values.append(y_test.to(device).detach().numpy())\n",
    "              error = self.loss_fn(y_test, yhat).item()\n",
    "              test_error.append(error)\n",
    "          mean_error = np.mean(test_error)\n",
    "          # self.error.append(mean_error)\n",
    "\n",
    "      return predictions, values,test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b29566-e2aa-46d6-a5b6-b675ebc9028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_dim': input_dim,\n",
    "    'hidden_dim' : hidden_dim,\n",
    "    'layer_dim' : layer_dim,\n",
    "    'output_dim' : output_dim,\n",
    "    'dropout_prob' : dropout\n",
    "}\n",
    "\n",
    "model = get_model('lstm', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay) \n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values, error = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)\n",
    "preds = np.concatenate(predictions, axis=0)\n",
    "vals = np.concatenate(values, axis=0)\n",
    "\n",
    "error=pd.Series(error)\n",
    "error.abs().mean()\n",
    "\n",
    "#Results dataFrame:\n",
    "predictions=pd.DataFrame(preds)\n",
    "predictions.rename(columns={0: 'y1_predicted', 1: 'y2_predicted'}, inplace=True)\n",
    "values=pd.DataFrame(vals)\n",
    "values.rename(columns={0: 'y1', 1: 'y2'}, inplace=True)\n",
    "\n",
    "results = pd.concat([values, predictions], axis=1)\n",
    "\n",
    "\n",
    "x_test_arr=pd.DataFrame(X_test_arr)\n",
    "x_test_arr.rename(columns={0: 'X1', 1: 'X2'}, inplace=True)\n",
    "\n",
    "frames = [x_test_arr,results] \n",
    "df_result = pd.concat(frames, axis=1)\n",
    "\n",
    "results\n",
    "\n",
    "df_result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db",
   "language": "python",
   "name": "db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
