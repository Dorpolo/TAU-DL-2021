{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0971b9f-9933-4489-978b-ef0193910d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn, optim as optim, functional as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed45c3-81bb-466a-93a0-d0d05fc7a543",
   "metadata": {},
   "source": [
    "# A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22d0e51-5258-4373-b5fb-f9501982e22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd472cf0290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "register_matplotlib_converters()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c52cdd-a1f4-4671-a080-aba04d5081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [(math.sqrt(99))/10, -1/10],\n",
    "    [1/10, (math.sqrt(99))/10]\n",
    "    ])\n",
    "\n",
    "B = np.array([\n",
    "    [math.sqrt(2)/2,-2/2],\n",
    "    [math.sqrt(2)/2,math.sqrt(2)/2]\n",
    "])\n",
    "\n",
    "\n",
    "eps1 = np.random.normal(loc=0, scale=0.2, size=[2, 2000])\n",
    "eps2 = np.random.normal(loc=0, scale=0.2**2, size=[2, 2000])\n",
    "\n",
    "\n",
    "x0 = np.array([1,0])+eps1[:,0] #do we need to add noise to XO?\n",
    "y0 = B.dot(x0) + eps2[:,0] #not true- how do w know the real YO?\n",
    "\n",
    "length = 2000\n",
    "train_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "x, y_hat = [x0], [y0]\n",
    "\n",
    "for i in range (length-1):\n",
    "  # X(k+1) = AX(k) + ϵ1(k+1)\n",
    "  x_next = a.dot(x[i]) + eps1[:, i+1]\n",
    "  x.append(x_next)\n",
    "  #Y(k+1) = BX(k+1) + ϵ2(k+1)\n",
    "  y_next = b.dot(x_next) + eps2[:, i+1]\n",
    "  y_hat.append(y_next)\n",
    "  i = i+1\n",
    "\n",
    "#need to update x_hat[i] accordingly\n",
    "X = pd.DataFrame(x)\n",
    "y = pd.DataFrame(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d9ac6-77b1-4347-9745-f4855a11059b",
   "metadata": {},
   "source": [
    "LSTM cells:\n",
    "\n",
    "\n",
    "*   C*t-1*- Cell state/Long-term memory- (last time stemp output)\n",
    "*   Hidden-State (Short term memmory)- H*t-1*\n",
    "*   Input- Y*t-1* (last time stemp output)\n",
    "\n",
    "\n",
    "INITILAZATIONS:\n",
    "H0,C0- tensors of Zeroes. in the backpopagation stage the network will learn these states by vy theire deriviations according to the loss functions.\n",
    "\n",
    "Stages:\n",
    "1.  Forget gate- Values between 0 to 1=> sigmoid(W *forget* (H*t-1*,Y*t-1*)+BIAS *forget* ) * C*t-1*\n",
    "\n",
    "2.  Input gate- - Values between -1 to 1==> sigmoid(Wi1(H*t-1*,Y*t-1*)+BIASi1) * tanjh(Wi2(H*t-1*,Y*t-1*)+BIASi2)\n",
    "\n",
    ">>The outcome represents the information to be kept in the long-term memory and used as the output. As the layer is being trained through back-propagation, the weights in the sigmoid function will be updated such that it learns to only let the useful pass through while discarding the less critical features\n",
    "\n",
    "3.  1+2: will define our C**t** value (New Long term memmory)\n",
    "\n",
    "4.  sigmoid(W output1 (H*t-1*,Y*t-1*)+BIAS output1) * tan (W output1(C**t**) + BIAS output2)- will define our output (Y_hat **t**) and out new Hidden state (H_hat **t**)  \n",
    ">First, the previous short-term memory and current input (H*t-1*,Y*t-1*) will be passed into a sigmoid function (Yes, this is the 3rd time we’re doing this) with different weights yet again to create the third and final filter. Then, we put the new long-term memory (C**t**) through an activation tanh function. The output from these 2 processes will be multiplied to produce the new short-term memory ( H**t**). The short-term ( H**t**) and long-term memory (C**t**) produced by these gates will then be carried over to the next cell for the process to be repeated. The output of each time step can be obtained ( Y**t**) from the short-term memory, also known as the hidden state.\n",
    "\n",
    "#https://towardsdatascience.com/building-rnn-lstm-and-gru-for-time-series-using-pytorch-a46e5b094e7b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc01e2-1885-422f-8433-51235003e724",
   "metadata": {},
   "source": [
    "## Building the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1f3108-0efb-4b77-b9aa-98791149a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_val_test_split(X, y,val_ratio, test_ratio):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X , y, 0.2, 0.5)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e395ae1-4871-44da-82e9-1d5846a5d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = len(y_train.columns)\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "batch_size =50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db",
   "language": "python",
   "name": "db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
