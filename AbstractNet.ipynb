{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebd7ff9-b64d-4300-a90f-91709e017cb6",
   "metadata": {},
   "source": [
    "# HW 1 - Building a network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653560dd-b61c-4ab1-a9d7-6465bc2d8e9f",
   "metadata": {},
   "source": [
    "* x1, x2, x3 = (1, 2, -1)\n",
    "* Weights have been initated with 1. value\n",
    "* Biases have been initated with 0 value\n",
    "* The reuqired network must include 2 hidden layers, with 2 neurons in each layer\n",
    "* One dimension output layer\n",
    "* true y value = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f379af-803f-458c-ada3-ca9ed8d6cd0e",
   "metadata": {},
   "source": [
    "# Loading required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc4caa-197f-4349-a802-b99d4da8eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239edf23-9d26-4e9c-89de-a2f41b7d6000",
   "metadata": {},
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dcaee7ab-e6f0-4b9e-b438-b6838b91ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.clip(x, 0, None)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def loss(y_pred: npt.ArrayLike, y_true: npt.ArrayLike):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "def d_loss(y_pred: npt.ArrayLike, y_true: npt.ArrayLike):\n",
    "    n = y_true.shape[0]\n",
    "    gradient = 2. * (y_pred - y_true) / n\n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800748b4-5c20-4d82-9460-95e678f2bab6",
   "metadata": {},
   "source": [
    "# The *Layer* class - OOP\n",
    "\n",
    " * The learning rate is set to 0.01\n",
    " * The activation function is ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "57b6da75-6594-4673-a86a-a38b2cb8adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    activationFunctions = {\n",
    "        'ReLU': (ReLU, d_ReLU),\n",
    "        'sigmoid': (sigmoid, d_sigmoid)\n",
    "    }\n",
    "    \n",
    "    lr = 0.1\n",
    "    \n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 inputs: int,\n",
    "                 activation: str = 'ReLU',\n",
    "                 init_w_with_1: bool = False):\n",
    "        \"\"\"\n",
    "        The layer instance should be initiated with the input count and number of neurons.\n",
    "        \"\"\"\n",
    "        if init_w_with_1:\n",
    "            self.W = np.ones((neurons, inputs))\n",
    "        else:\n",
    "            self.W = np.random.randn(neurons, inputs)\n",
    "        self.b = np.zeros((neurons, 1))\n",
    "        self.act, self.d_act = self.activationFunctions.get(activation)\n",
    "        \n",
    "    def forward(self, A_prev: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"\n",
    "        Each forward step will calculate the next layer neurin as a function of its prev input and current \n",
    "        weight dot product, on top of that, the activation function will be applied.\n",
    "        \"\"\"\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = np.dot(self.W, self.A_prev) + self.b\n",
    "        self.A = self.act(self.Z)\n",
    "        return self.A\n",
    "        \n",
    "    def backprop(self, dA: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        dZ = np.multiply(self.d_act(self.Z), dA)\n",
    "        dW = 1/dZ.shape[1] * np.dot(dZ, self.A_prev.T)\n",
    "        db = 1/dZ.shape[1] * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        self.W = self.W - self.lr * dW\n",
    "        self.b = self.b - self.lr * db\n",
    "        \n",
    "        return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f8e3f25c-db80-420f-a135-d69e1cd10109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      " \n",
      "**************************************************\n",
      "Layer 0: \n",
      " [[2.]\n",
      " [2.]]\n",
      "Layer 1: \n",
      " [[4.]\n",
      " [4.]]\n",
      "Layer 2: \n",
      " [[8.]]\n",
      "Epoch Cost 0: \n",
      " 64.0\n",
      "**************************************************\n",
      "Final param values:\n",
      "**************************************************\n",
      "\n",
      "Weight: [[ 4.2  7.4 -2.2]\n",
      " [ 4.2  7.4 -2.2]]\n",
      "Biases: [[3.2]\n",
      " [3.2]]\n"
     ]
    }
   ],
   "source": [
    "def logit(index, text, value = None, add_sep: bool = False):\n",
    "    print(f\"{text} {index}: \\n {value if value is not None else ''}\")\n",
    "    if add_sep:\n",
    "        print(f\"{'*'*50}\")\n",
    "\n",
    "# Given inputs\n",
    "\n",
    "x_train = np.array([[1.0], [2.0], [-1.0]])\n",
    "y_train = np.array([[0]]) \n",
    "\n",
    "m = 1\n",
    "epochs = 1\n",
    "\n",
    "layers = [Layer(2, 3, 'ReLU'), Layer(2, 2, 'ReLU'), Layer(1, 2, 'ReLU')]\n",
    "costs = []\n",
    "\n",
    "for i, epoch in enumerate(range(epochs)):\n",
    "    logit(i, \"Epoch\", add_sep=True)\n",
    "    # FORWARD\n",
    "    A = x_train\n",
    "    for j, layer in enumerate(layers):\n",
    "        A = layer.forward(A)\n",
    "        logit(j, \"Layer\", A)\n",
    "    \n",
    "    # COSTS\n",
    "    cost = loss(y_train, A)\n",
    "    logit(i, \"Epoch Cost\", cost)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    # BACKPROP\n",
    "    dA = d_loss(y_train, A)\n",
    "    for k, layer in enumerate(reversed(layers)):\n",
    "        dA = layer.backprop(dA)\n",
    "\n",
    "print(f\"{'*'*50}\\nFinal param values:\\n{'*'*50}\\n\")\n",
    "print(f\"Weight: {layer.W}\")\n",
    "print(f\"Biases: {layer.b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18827768-58e5-4add-a7e7-cb07bb1c2d97",
   "metadata": {},
   "source": [
    "### Abstract Class - Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "cd4d09e1-69e9-45b3-bca1-9b9d6daf2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def forward_prop(self, input):\n",
    "        pass\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def backward_prop(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b10a6b82-695d-4207-af8d-43acb7a7ba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.ones((3, 2)) \n",
    "weights\n",
    "# # np.random.rand(1, output_size) - 0.5\n",
    "bias = np.ones((1, 1)) \n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6582858a-45f7-46ea-a969-d202eda93cb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recall - \n",
    "\n",
    "    * DE/DX = (DE/DY)*W.T\n",
    "    * DE/DW = X.T*(DE/DY)\n",
    "    * DE/DB = DE/DY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913a825-9f1a-4fdc-9fa3-9db3a53f44fd",
   "metadata": {},
   "source": [
    "### Fully Connected && Activation Layer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "8eeb0644-5f9f-4c82-8e2b-3f94a3d2069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, d_activation):\n",
    "        self.activation = activation\n",
    "        self.d_activation = d_activation\n",
    "    \n",
    "    def forward_prop(self, input_data: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_prop(self, output_err: npt.ArrayLike, lr) -> npt.ArrayLike:\n",
    "        return self.d_activation(self.input) * output_err\n",
    "    \n",
    "    \n",
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, hw_1_init: bool = False):\n",
    "        if hw_1_init:\n",
    "            self.weights = np.ones((input_size, output_size)) \n",
    "            self.bias = np.zeros((1, 1))\n",
    "        else:\n",
    "            self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "            self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        \n",
    "        \n",
    "    def forward_prop(self, input_data: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "        \n",
    "    def backward_prop(self, output_err: npt.ArrayLike, lr: float) -> npt.ArrayLike:\n",
    "        input_err = np.dot(output_err, self.weights.T)\n",
    "        weights_err = np.dot(self.input.T, output_err)\n",
    "        \n",
    "        self.weights -= lr * weights_err\n",
    "        self.bias -= lr * output_err\n",
    "        return input_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6b2f8-b53b-4844-ba3b-e660cc05ffbb",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "4cc01e26-9b36-416e-8909-05a49a650de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.d_loss = None\n",
    "    \n",
    "    def add(self, layer: Layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def use(self, loss, d_loss):\n",
    "        self.loss = loss\n",
    "        self.d_loss = d_loss\n",
    "    \n",
    "    def predict(self, input_data: npt.ArrayLike) -> list:\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "    \n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_prop(output)\n",
    "            result.append(output)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def fit(self, x_train: npt.ArrayLike, y_train: npt.ArrayLike, epochs: int, lr: float):\n",
    "        samples = len(x_train)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_prop(output)\n",
    "                \n",
    "                err += self.loss(y_train[j], output)\n",
    "                \n",
    "                error = self.d_loss(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_prop(error, lr)\n",
    "            \n",
    "            err /= samples\n",
    "            print(f\"Epoch {i+1}/{epochs} >> error={err}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bc3fb-6c3c-4377-a1e9-c269078f114e",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "a09c6395-de88-4ea9-8990-70ad69cdac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.clip(x, 0, None)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def d_mse(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf754fb-264a-4706-9de2-7f00682c1a7d",
   "metadata": {},
   "source": [
    "### My Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "ae4d1fbb-04b9-4b96-94d6-a30f71ab7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_FUNCTIONS = {\n",
    "    'ReLU': (ReLU, d_ReLU),\n",
    "    'sigmoid': (sigmoid, d_sigmoid)\n",
    "}\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a0580-0cd3-4c4a-ba5d-1a517c09869e",
   "metadata": {},
   "source": [
    "### Network Inititation - General Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "7d796bc0-ed48-4dc6-a742-0eaa6dec6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 >> error=0.5\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "net = Network()\n",
    "net.add(FullyConnectedLayer(2, 3))\n",
    "net.add(ActivationLayer(*ACT_FUNCTIONS['ReLU']))\n",
    "net.add(FullyConnectedLayer(3, 1))\n",
    "net.add(ActivationLayer(*ACT_FUNCTIONS['ReLU']))\n",
    "\n",
    "net.use(mse, d_mse)\n",
    "net.fit(x_train, y_train, epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa2a9b-6bb4-413b-af35-bf0d07d515bb",
   "metadata": {},
   "source": [
    "### Network Inititation - Home Work I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "fc18ae11-01ee-4f27-9343-47bad9c1b197",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (3,2) not aligned: 1 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/6r5p1rqs5kn81svbrkhbrjcc0000gn/T/ipykernel_50832/2385198530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnet_hw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet_hw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_hw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_hw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lp/6r5p1rqs5kn81svbrkhbrjcc0000gn/T/ipykernel_50832/714583382.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epochs, lr)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lp/6r5p1rqs5kn81svbrkhbrjcc0000gn/T/ipykernel_50832/3820852745.py\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (3,2) not aligned: 1 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "x_train_hw = np.array([[1], [2], [-1]])\n",
    "y_train_hw = np.array([[0]])\n",
    "\n",
    "\n",
    "net_hw1 = Network()\n",
    "net_hw1.add(FullyConnectedLayer(3, 2, hw_1_init=True))\n",
    "net_hw1.add(ActivationLayer(*ACT_FUNCTIONS['ReLU']))\n",
    "net_hw1.add(FullyConnectedLayer(2, 2, hw_1_init=True))\n",
    "net_hw1.add(ActivationLayer(*ACT_FUNCTIONS['ReLU']))\n",
    "\n",
    "net_hw1.use(mse, d_mse)\n",
    "net_hw1.fit(x_train_hw, y_train_hw, epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "85867b48-d220-4e18-be27-dfc4194833fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1764067 ,  0.09390614],\n",
       "       [ 0.00358406, -0.46632579],\n",
       "       [ 0.33714902,  0.00653893]])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(input_size, output_size) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "144bf4df-21a6-43e4-b713-3fbf24241480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.]])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 3\n",
    "output_size = 2\n",
    "x_train_hw = np.array([[1], [2], [-1]]).T\n",
    "np.dot(x_train_hw,  np.ones((input_size, output_size))) + np.ones((1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "b06ab881-4980-4d7b-9f51-f660553ba10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_hw.shape\n",
    "np.ones((input_size, output_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "bf55b28e-cbb7-49e9-b062-63d9b36fe040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "5cb0e68f-502c-4b87-acd0-9ec7630ae601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2.]])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x_train_hw.T,  np.ones((input_size, output_size))) + np.zeros((1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
